<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns:ssh="uri:oozie:ssh-action:0.2" xmlns:shell="uri:oozie:shell-action:0.3" xmlns="uri:oozie:workflow:0.5" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" name="pde-binfile-workflow">
	<parameters>
		<property>
			<name>HDFS_APPLIB</name>
			<value>hdfs://192.85.247.104:19000/user/dbadmin/share/lib/preload/lib</value>
		</property>
		<property>
			<name>PDE_APPLIB</name>
			<value>hdfs://192.85.247.104:19000/user/dbadmin/pde2/lib</value>
		</property>
	</parameters>
	<global>
		<configuration>
			<property>
				<name>oozie.launcher.yarn.app.mapreduce.am.env</name>
				<value>SPARK_HOME=/data/spark-2.0.0-bin-hadoop2.7/</value>
			</property>
		</configuration>
	</global>
	<start to="SparkProcess"/>
	<kill name="fail">
		<message>Java failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
	</kill>
	<action name="SparkProcess">
		<spark xmlns="uri:oozie:spark-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <master>yarn</master>
            <mode>cluster</mode>
            <name>${wfName}</name>
            <class>etl.engine.ETLCmdMain</class>
            <jar>${HDFS_APPLIB}/bdap.engine-r0.4.0.jar</jar>
            <spark-opts>--executor-memory 2G --num-executors 4 --conf spark.yarn.historyServer.address=http://192.85.247.104:18080 --conf spark.eventLog.dir=hdfs://192.85.247.104:19000/spark/logs --conf spark.eventLog.enabled=true --jars ${HDFS_APPLIB}/commons-cli-1.3.1.jar,${HDFS_APPLIB}/commons-codec-1.4.jar,${HDFS_APPLIB}/commons-collections-3.2.1.jar,${HDFS_APPLIB}/commons-compress-1.4.1.jar,${HDFS_APPLIB}/commons-configuration-1.7.jar,${HDFS_APPLIB}/commons-csv-1.4.jar,${HDFS_APPLIB}/commons-daemon-1.0.13.jar,${HDFS_APPLIB}/commons-dbcp-1.4.jar,${HDFS_APPLIB}/commons-el-1.0.jar,${HDFS_APPLIB}/commons-exec-1.2.jar,${HDFS_APPLIB}/commons-httpclient-3.1.jar,${HDFS_APPLIB}/commons-io-2.5.jar,${HDFS_APPLIB}/commons-lang-2.6.jar,${HDFS_APPLIB}/commons-lang3-3.3.2.jar,${HDFS_APPLIB}/commons-logging-1.1.3.jar,${HDFS_APPLIB}/commons-math3-3.1.1.jar,${HDFS_APPLIB}/commons-net-3.1.jar,${HDFS_APPLIB}/commons-pool-1.5.4.jar,${HDFS_APPLIB}/jackson-annotations-2.6.0.jar,${HDFS_APPLIB}/jackson-core-2.6.5.jar,${HDFS_APPLIB}/jackson-databind-2.6.5.jar,${HDFS_APPLIB}/jsch-0.1.53.jar,${HDFS_APPLIB}/kafka-clients-0.10.0.1.jar,${HDFS_APPLIB}/log4j-api-2.6.2.jar,${HDFS_APPLIB}/log4j-core-2.6.2.jar,${HDFS_APPLIB}/vertica-jdbc-7.0.1-0.jar,${HDFS_APPLIB}/spark-streaming-kafka-0-10_2.11-2.0.0.jar,${HDFS_APPLIB}/bdap.common-r0.4.0.jar,${PDE_APPLIB}/vprj.pde-r0.4.0.jar</spark-opts>
            <arg>hpe.pde.etl.PdeSparkFlow</arg>
            <arg>${wfName}</arg>
            <arg>${wf:id()}</arg>
            <arg>action.spark.properties</arg>
            <arg>${nameNode}</arg>
        </spark>
		<ok to="end"/>
		<error to="fail"/>
	</action>
	<end name="end"/>
</workflow-app>
