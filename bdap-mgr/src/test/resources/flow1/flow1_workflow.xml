<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<workflow-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:shell="uri:oozie:shell-action:0.3" xmlns:ssh="uri:oozie:ssh-action:0.2" xmlns="uri:oozie:workflow:0.5" name="flow1">
    <start to="sftp"/>
    <kill name="fail">
        <message>failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <action name="sftp">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduces</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/flow1/sftpcfg/test1.sftp.map.properties</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.NLineInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.NullOutputFormat</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.SftpCmd</value>
                </property>
                <property>
                    <name>wfName</name>
                    <value>flow1</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>action_sftp.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="sftp_fork"/>
        <error to="fail"/>
    </action>
    <fork name="sftp_fork">
        <path start="d2xml2csv"/>
        <path start="d1csvtransform"/>
    </fork>
    <action name="d1csvtransform">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/flow1/d1csvtrans/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeReducer</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/flow1/data1/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/flow1/d1csvtrans/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.CsvTransformCmd</value>
                </property>
                <property>
                    <name>wfName</name>
                    <value>flow1</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>action_d1csvtransform.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="csvmerge_join"/>
        <error to="fail"/>
    </action>
    <action name="d2xml2csv">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/flow1/d2csvtrans/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeReducer</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/flow1/data2/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>etl.input.CombineXmlInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.split.maxsize</name>
                    <value>268435456</value>
                </property>
                <property>
                    <name>xmlinput.start</name>
                    <value>&lt;info</value>
                </property>
                <property>
                    <name>xmlinput.end</name>
                    <value>&lt;/info&gt;</value>
                </property>
                <property>
                    <name>xmlinput.row.start</name>
                    <value>&lt;values</value>
                </property>
                <property>
                    <name>xmlinput.row.end</name>
                    <value>&lt;/values&gt;</value>
                </property>
                <property>
                    <name>xmlinput.row.max.number</name>
                    <value>3000</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/flow1/d2csvtrans/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.flow.test.cmd.Flow1Xml2CsvCmd</value>
                </property>
                <property>
                    <name>wfName</name>
                    <value>flow1</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>action_d2xml2csv.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="csvmerge_join"/>
        <error to="fail"/>
    </action>
    <join name="csvmerge_join" to="csvmerge"/>
    <action name="csvmerge">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/flow1/csvmerge/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeReducer</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/flow1/d1csvtrans/${wf:id()},/flow1/d2csvtrans/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/flow1/csvmerge/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.output.textoutputformat.separator</name>
                    <value>,</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.CsvAggregateCmd</value>
                </property>
                <property>
                    <name>wfName</name>
                    <value>flow1</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>action_csvaggr.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="csvsave"/>
        <error to="fail"/>
    </action>
    <action name="csvsave">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeReducer</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/flow1/csvmerge/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.NullOutputFormat</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.SaveDataCmd</value>
                </property>
                <property>
                    <name>wfName</name>
                    <value>flow1</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>action_csvsave.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>
    <end name="end"/>
</workflow-app>

