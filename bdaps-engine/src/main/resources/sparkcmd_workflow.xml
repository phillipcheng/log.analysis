<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns:ssh="uri:oozie:ssh-action:0.2" xmlns:shell="uri:oozie:shell-action:0.3" xmlns="uri:oozie:workflow:0.5" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" name="[[workflowName]]">
	<parameters>
		<property>
			<name>BDAP_LIB</name>
			<value>${nameNode}/bdap-r0.5.0/engine/lib</value>
		</property>
		<property>
			<name>APP_LIB</name>
			<value>${nameNode}${prjFolder}${wfName}/lib</value>
		</property>
		<property>
			<name>BDAP_LIB_JARS</name>
			<value>${engineJars}</value>
		</property>
	</parameters>
	<global>
		<configuration>
			<property>
				<name>oozie.launcher.yarn.app.mapreduce.am.env</name>
				<value>SPARK_HOME=[[sparkhome]]</value>
			</property>
		</configuration>
	</global>
	<start to="SparkProcess"/>
	<kill name="fail">
		<message>Java failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
	</kill>
	<action name="SparkProcess">
		<shell xmlns="uri:oozie:shell-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>submitspark.sh</exec>
            <argument>--class</argument>
            <argument>etl.engine.ETLCmdMain</argument>
            <argument>--master</argument>
            <argument>yarn</argument>
            <argument>--deploy-mode</argument>
            <argument>cluster</argument>
            <argument>--num-executors</argument>
            <argument>4</argument>
            <argument>--executor-memory</argument>
            <argument>7G</argument>
            <argument>--total-executor-cores</argument>
            <argument>7</argument>
            <argument>--conf</argument>
            <argument>spark.yarn.historyServer.address=[[sparkhistoryserver]]</argument>
            <argument>--conf</argument>
            <argument>spark.eventLog.dir=[[defaultfs]]/spark/logs</argument>
            <argument>--conf</argument>
            <argument>spark.eventLog.enabled=true</argument>
            <argument>--conf</argument>
            <argument>spark.driver.userClassPathFirst=true</argument>
            <argument>--conf</argument>
            <argument>spark.executor.userClassPathFirst=true</argument>
            <argument>--jars</argument>
            <argument>${BDAP_LIB}/commons-cli-1.3.1.jar,${BDAP_LIB}/commons-codec-1.4.jar,${BDAP_LIB}/commons-collections-3.2.2.jar,${BDAP_LIB}/commons-compress-1.12.jar,${BDAP_LIB}/commons-configuration-1.7.jar,${BDAP_LIB}/commons-csv-1.4.jar,${BDAP_LIB}/commons-daemon-1.0.13.jar,${BDAP_LIB}/commons-dbcp-1.4.jar,${BDAP_LIB}/commons-exec-1.2.jar,${BDAP_LIB}/commons-httpclient-3.1.jar,${BDAP_LIB}/commons-io-2.5.jar,${BDAP_LIB}/commons-lang-2.6.jar,${BDAP_LIB}/commons-lang3-3.5.jar,${BDAP_LIB}/commons-logging-1.1.3.jar,${BDAP_LIB}/commons-math3-3.1.1.jar,${BDAP_LIB}/commons-net-3.1.jar,${BDAP_LIB}/commons-pool-1.5.4.jar,${BDAP_LIB}/jackson-annotations-2.6.0.jar,${BDAP_LIB}/jackson-core-2.6.5.jar,${BDAP_LIB}/jackson-databind-2.6.5.jar,${BDAP_LIB}/jsch-0.1.53.jar,${BDAP_LIB}/kafka-clients-0.10.0.1.jar,${BDAP_LIB}/log4j-api-2.6.2.jar,${BDAP_LIB}/log4j-core-2.6.2.jar,${BDAP_LIB}/vertica-jdbc-7.0.1-0.jar,${BDAP_LIB}/parquet-hadoop-bundle-1.8.1.jar,${BDAP_LIB}/bdap.common-r0.5.0.jar,${APP_LIB}/${wfName}.jar[[thirdpartyjars]]</argument>
            <argument>${BDAP_LIB}/bdap.engine-r0.5.0.jar</argument>
            <argument>${cmdClassName}</argument>
            <argument>${wfName}</argument>
            <argument>${wf:id()}</argument>
            <argument>action.spark.properties</argument>
            <argument>${nameNode}</argument>
        </shell>
		<ok to="end"/>
		<error to="fail"/>
	</action>
	<end name="end"/>
</workflow-app>
